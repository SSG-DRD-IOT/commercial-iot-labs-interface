<div class="row wrapper border-bottom white-bg page-heading">
    <div class="col-lg-10">
        <h2>Face detection using OpenVINO&trade;</h2>
        <ol class="breadcrumb">
            <li>
                <a href="index.html">Home</a>
            </li>
            <li>
                <a>Labs</a>
            </li>
            <li>
                OpenVINO&trade; Sample
            </li>
            <li class="active">
                <strong>Face detection using OpenVINO&trade;</strong>
            </li>
        </ol>
    </div>
</div>

<div class="wrapper wrapper-content animated fadeInRight" ng-controller="CodeEditorCtrl">
          <div ibox title="Objectives">
		  <div content-block name="xdk-issues" message="Check after completion of Lab Overview" image-link="../views/labs/analytics-openvinoface/faceDet_Class.png">
             
              <b>Lab Overview</b>
              <ul>
			  <li>Here we will be working with TODOs for face detection only, keep all other TODOs as it is. We will be using them in our next labs.</li>
			  <li>We will be replacing Face detection TODOs with the following:</li>
			  </ul>
			  
			</div>	
             <div content-block name="xdk-issues" message="Check after understanding task for this Lab" image-link="../views/labs/analytics-openvinoagegenderdetection/images/FlowChart.png">   
				<p>We will be replacing Age and Gender detection TODOs with the following:</p>
				<ul>
				<li>Include the required header files.</li>
				<li>Capture video frames using OpenCV API.</li>
				<li>Loading inference engine plugins</li>
				<li>Loading pre trained optimized face detection model</li>
				<li>Request for inference on GPU</li>
				<li>With the inference result, we will draw a rectangle marking the face</li>
				</ul>                                            
              </div>
          </div>

    

          <div ibox title="Include required headers">
                <div content-block name="xdk-issues" message="Check after including required headers">
                 <p>We need to include required header files for inferencing with OpenVINO&trade;</p> 
				  <ul>
					<li>Replace first #TODO: Face detection 1</li>
					<li>Paste the following lines</li>
					</ul>
                    
				 <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">                    					
					#include &ltinference_engine.hpp&gt
					#include "interactive_face_detection.hpp"
					#include "mkldnn/mkldnn_extension_ptr.hpp"
					#include &ltopencv2/opencv.hpp&gt
                    </ui-codemirror>
              </div>
            </div>

          <div ibox title="Capture video frames">
                <div content-block name="xdk-issues" message="Check after completing Input processing">
				<p>We need to capture video frames using OpenCV APIs</p>
				 <ul>
					<li>Replace second #TODO: Face detection 2</li>
					<li>Paste the following lines</li>
					</ul>
                                    
                  <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
                    //If there is a single camera connected, just pass 0.
					cv::VideoCapture cap;
					cap.open(0);

					cv::Mat frame;
					cap.read(frame);
                </ui-codemirror>
              
              </div>
            </div>

          <div ibox title="Select GPU for inferencing face detection">
                <div content-block name="xdk-issues" message="Check after slecting GPU as plugin device">
				<p>Select the plugins device for inference engine where we want to run our inferencing</p>
				<ul>
					<li>Replace third #TODO: Face detection 3</li>
					<li>Paste the following lines</li>
					</ul>
                  </br>
                    <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
						//Select plugins for inference engine
						std::map<std::string, InferencePlugin> pluginsForDevices;

						//Select GPU as plugin device to load Face Detection pre trained optimized model
						InferencePlugin plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("GPU");
						pluginsForDevices["GPU"] = plugin;
                    </ui-codemirror>
                  </div>
            </div>
			<div ibox title="Load pre-trained optimized data on GPU">
			<p>The pre-trained model XML files have been optimized and generated by OpenVINO™ model optimizer from OpenVINO™ toolkit pre-trained models folder. However, participants are expected to experiment with different pre trained models available.</p>
			<ul>
				<li>Replace fourth: Face detection 4</li>
				<li>Paste the following lines</li>
			</ul>

                <div content-block name="xdk-issues" message="Check after completion of Load xml files for face,age and gender detection">
                  </br>
                    <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
                   //Load pre trained optimized data model for face detection 
	FLAGS_Face_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\intel_models\\face-detection-adas-0001\\FP32\\face-detection-adas-0001.xml";

	//Load Face Detection model to target device
	FaceDetectionClass FaceDetection;
	Load(FaceDetection).into(pluginsForDevices["GPU"]);

                    </ui-codemirror>
                  </div>
            </div>

          <div ibox title="Main loop for inferencing">
		  <p>Here we are populating the face detection object for Inference request, and after getting result we will draw rectangular box around the face. Also we will use OpenCV APIs for a display window and exit the window</p>
		  <ul>
			<li>Replace fifth #TODO: Face detection 5</li>
			<li>Paste the following lines</li>
			
		</ul>
                <div content-block name="xdk-issues" message="Check after completion of Main inferencing loop">
                </br>
                  <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
       // Main inference loop	
	while (true) {
		//Grab the next frame from camera and populate Inference Request
		cap.grab();
		FaceDetection.enqueue(frame);

		//Submit Inference Request for face detection and wait for result
		FaceDetection.submitRequest();
		FaceDetection.wait();

		//TODO: Age and Gender Detection 4

		FaceDetection.fetchResults();

		//TODO: Age and Gender Detection 5

		for (auto & result : FaceDetection.results) {
			cv::Rect rect = result.location;

			//TODO: Age and Gender Detection 6

			// Giving same colour to male and female
			auto rectColor = cv::Scalar(0, 255, 0);

			cv::rectangle(frame, result.location, rectColor, 1);			
		}

		if (-1 != cv::waitKey(1))
			break;

		cv::imshow("Detection results", frame);

		if (!cap.retrieve(frame)) {
			break;
		}
		//TODO: Cloud integration 2

                </ui-codemirror>
              </div>
            </div>
			<div ibox title="Here is the final solution">
		  <p>Keep the TODO as it is. We will re-use this program during Age and Gender detection</p>
		  
                <div content-block name="xdk-issues" message="Check after completion of Main inferencing loop">
                </br>
				 <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
#include &ltgflags/gflags.h&gt
#include &ltfunctional&gt
#include &ltiostream&gt
#include &ltfstream&gt
#include &ltrandom&gt
#include &ltmemory&gt
#include &ltchrono&gt
#include &ltstring&gt
#include &ltutility&gt
#include &ltalgorithm&gt
#include &ltiterator&gt
#include &ltsamples/common.hpp&gt
#include &ltsamples/slog.hpp&gt
#include &ltext_list.hpp&gt
#include &ltsstream&gt
#include &ltmap&gt
#include &ltvector&gt
#include "mkldnn/mkldnn_extension_ptr.hpp"
#include &ltinference_engine.hpp&gt
#include "interactive_face_detection.hpp"
#include &ltopencv2/opencv.hpp&gt

using namespace InferenceEngine;


template <typename T>
void matU8ToBlob(const cv::Mat& orig_image, Blob::Ptr& blob, float scaleFactor = 1.0, int batchIndex = 0) {
	SizeVector blobSize = blob.get()->dims();
	const size_t width = blobSize[0];
	const size_t height = blobSize[1];
	const size_t channels = blobSize[2];

	T* blob_data = blob->buffer().as<T*>();

	cv::Mat resized_image(orig_image);
	if (width != orig_image.size().width || height != orig_image.size().height) {
		cv::resize(orig_image, resized_image, cv::Size(width, height));
	}

	int batchOffset = batchIndex * width * height * channels;

	for (size_t c = 0; c < channels; c++) {
		for (size_t h = 0; h < height; h++) {
			for (size_t w = 0; w < width; w++) {
				blob_data[batchOffset + c * width * height + h * width + w] =
					resized_image.at<cv::Vec3b>(h, w)[c] * scaleFactor;
			}
		}
	}
}

// -------------------------Generic routines for detection networks-------------------------------------------------

struct BaseDetection {
	ExecutableNetwork net;
	InferenceEngine::InferencePlugin * plugin;
	InferRequest::Ptr request;
	std::string & commandLineFlag;
	std::string topoName;
	const int maxBatch;

	BaseDetection(std::string &commandLineFlag, std::string topoName, int maxBatch)
		: commandLineFlag(commandLineFlag), topoName(topoName), maxBatch(maxBatch) {}

	virtual ~BaseDetection() {}

	ExecutableNetwork* operator ->() {
		return &net;
	}
	virtual InferenceEngine::CNNNetwork read() = 0;

	virtual void submitRequest() {
		if (!enabled() || request == nullptr) return;
		request->StartAsync();
	}

	virtual void wait() {
		if (!enabled() || !request) return;
		request->Wait(IInferRequest::WaitMode::RESULT_READY);
	}
	mutable bool enablingChecked = false;
	mutable bool _enabled = false;

	bool enabled() const {
		if (!enablingChecked) {
			_enabled = !commandLineFlag.empty();

			enablingChecked = true;
		}
		return _enabled;
	}
	void printPerformanceCounts() {
		if (!enabled()) {
			return;
		}

	}
};

struct FaceDetectionClass : BaseDetection {
	std::string input;
	std::string output;
	int maxProposalCount;
	int objectSize;
	int enquedFrames = 0;
	float width = 0;
	float height = 0;
	bool resultsFetched = false;
	std::vector<std::string> labels;
	using BaseDetection::operator=;

	struct Result {
		int label;
		float confidence;
		cv::Rect location;
	};

	std::vector<Result> results;

	void submitRequest() override {
		if (!enquedFrames) return;
		enquedFrames = 0;
		resultsFetched = false;
		results.clear();
		BaseDetection::submitRequest();
	}

	void enqueue(const cv::Mat &frame) {
		//if (!enabled()) return;

		if (!request) {
			request = net.CreateInferRequestPtr();
		}

		width = frame.cols;
		height = frame.rows;

		auto  inputBlob = request->GetBlob(input);

		matU8ToBlob<uint8_t >(frame, inputBlob);

		enquedFrames = 1;
	}

	FaceDetectionClass() : BaseDetection(FLAGS_Face_Model, "Face Detection", 1) {}
	InferenceEngine::CNNNetwork read() override {
		InferenceEngine::CNNNetReader netReader;
		/** Read network model **/
		netReader.ReadNetwork(FLAGS_Face_Model);
		/** Set batch size to 1 **/
		netReader.getNetwork().setBatchSize(maxBatch);
		/** Extract model name and load it's weights **/
		std::string binFileName = fileNameNoExt(FLAGS_Face_Model) + ".bin";
		netReader.ReadWeights(binFileName);
		/** Read labels (if any)**/
		std::string labelFileName = fileNameNoExt(FLAGS_Face_Model) + ".labels";

		std::ifstream inputFile(labelFileName);
		std::copy(std::istream_iterator<std::string>(inputFile),
			std::istream_iterator<std::string>(),
			std::back_inserter(labels));

		/** SSD-based network should have one input and one output **/
		// ---------------------------Check inputs ------------------------------------------------------
		InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());
		auto& inputInfoFirst = inputInfo.begin()->second;
		inputInfoFirst->setPrecision(Precision::U8);
		inputInfoFirst->getInputData()->setLayout(Layout::NCHW);

		// ---------------------------Check outputs ------------------------------------------------------
		InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

		auto& _output = outputInfo.begin()->second;
		output = outputInfo.begin()->first;

		const auto outputLayer = netReader.getNetwork().getLayerByName(output.c_str());

		const int num_classes = outputLayer->GetParamAsInt("num_classes");

		const InferenceEngine::SizeVector outputDims = _output->dims;
		maxProposalCount = outputDims[1];
		objectSize = outputDims[0];

		_output->setPrecision(Precision::FP32);
		_output->setLayout(Layout::NCHW);


		input = inputInfo.begin()->first;
		return netReader.getNetwork();
	}

	void fetchResults() {
		if (!enabled()) return;
		results.clear();
		if (resultsFetched) return;
		resultsFetched = true;
		const float *detections = request->GetBlob(output)->buffer().as<float *>();

		for (int i = 0; i < maxProposalCount; i++) {
			float image_id = detections[i * objectSize + 0];
			Result r;
			r.label = static_cast<int>(detections[i * objectSize + 1]);
			r.confidence = detections[i * objectSize + 2];
			if (r.confidence <= FLAGS_t) {
				continue;
			}

			r.location.x = detections[i * objectSize + 3] * width;
			r.location.y = detections[i * objectSize + 4] * height;
			r.location.width = detections[i * objectSize + 5] * width - r.location.x;
			r.location.height = detections[i * objectSize + 6] * height - r.location.y;

			if (image_id < 0) {
				break;
			}
			if (FLAGS_r) {
				std::cout << "[" << i << "," << r.label << "] element, prob = " << r.confidence <<
					"    (" << r.location.x << "," << r.location.y << ")-(" << r.location.width << ","
					<< r.location.height << ")"
					<< ((r.confidence > FLAGS_t) ? " WILL BE RENDERED!" : "") << std::endl;
			}

			results.push_back(r);
		}
	}
};

struct AgeGenderDetection : BaseDetection {
	std::string input;
	std::string outputAge;
	std::string outputGender;
	int enquedFaces = 0;

	using BaseDetection::operator=;
	AgeGenderDetection() : BaseDetection(FLAGS_Age_Gender_Model, "Age Gender", FLAGS_n_ag) {}

	void submitRequest() override {
		if (!enquedFaces) return;
		BaseDetection::submitRequest();
		enquedFaces = 0;
	}

	void enqueue(const cv::Mat &face) {

		if (!request) {
			request = net.CreateInferRequestPtr();
		}

		auto  inputBlob = request->GetBlob(input);
		matU8ToBlob<float>(face, inputBlob, 1.0f, enquedFaces);
		enquedFaces++;
	}

	struct Result { float age; float maleProb; };
	Result operator[] (int idx) const {
		auto  genderBlob = request->GetBlob(outputGender);
		auto  ageBlob = request->GetBlob(outputAge);

		return{ ageBlob->buffer().as<float*>()[idx] * 100,
			genderBlob->buffer().as<float*>()[idx * 2 + 1] };
	}

	CNNNetwork read() override {

		InferenceEngine::CNNNetReader netReader;
		/** Read network model **/
		netReader.ReadNetwork(FLAGS_Age_Gender_Model);

		//	/** Set batch size to 16 
		netReader.getNetwork().setBatchSize(16);
		//slog::info << "Batch size is set to " << netReader.getNetwork().getBatchSize() << " for Age Gender" << slog::endl;**/

		/** Extract model name and load it's weights **/
		std::string binFileName = fileNameNoExt(FLAGS_Age_Gender_Model) + ".bin";
		netReader.ReadWeights(binFileName);

		/** Age Gender network should have one input two outputs **/
		InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());

		auto& inputInfoFirst = inputInfo.begin()->second;
		inputInfoFirst->setPrecision(Precision::FP32);
		inputInfoFirst->getInputData()->setLayout(Layout::NCHW);
		input = inputInfo.begin()->first;

		// ---------------------------Check outputs ------------------------------------------------------
		InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

		auto it = outputInfo.begin();
		auto ageOutput = (it++)->second;
		auto genderOutput = (it++)->second;

		outputAge = ageOutput->name;
		outputGender = genderOutput->name;
		_enabled = true;
		return netReader.getNetwork();
	}
};

// ***** For loading xml and bin files
struct Load {
	BaseDetection& detector;
	explicit Load(BaseDetection& detector) : detector(detector) { }

	void into(InferenceEngine::InferencePlugin & plg) const {
		if (detector.enabled()) {
			detector.net = plg.LoadNetwork(detector.read(), {});
			detector.plugin = &plg;
		}
	}
};

int main(int argc, char *argv[]) {
	//TODO: Age and Gender Detection 1
	//TODO: Cloud integration 1

	//If there is a single camera connected, just pass 0.
	cv::VideoCapture cap;
	cap.open(0);

	cv::Mat frame;
	cap.read(frame);

	//Select plugins for inference engine
	std::map<std::string, InferencePlugin> pluginsForDevices;

	//Select GPU as plugin device to load Face Detection pre trained optimized model
	InferencePlugin plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("GPU");
	pluginsForDevices["GPU"] = plugin;

	//TODO: Age and Gender Detection 2


	//Load pre trained optimized data model for face detection 
	FLAGS_Face_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\intel_models\\face-detection-adas-0001\\FP32\\face-detection-adas-0001.xml";

	//Load Face Detection model to target device
	FaceDetectionClass FaceDetection;
	Load(FaceDetection).into(pluginsForDevices["GPU"]);


	//TODO: Age and Gender Detection 3


	// Main inference loop	
	while (true) {
		//Grab the next frame from camera and populate Inference Request
		cap.grab();
		FaceDetection.enqueue(frame);

		//Submit Inference Request for face detection and wait for result
		FaceDetection.submitRequest();
		FaceDetection.wait();

		//TODO: Age and Gender Detection 4

		FaceDetection.fetchResults();

		//TODO: Age and Gender Detection 5

		for (auto & result : FaceDetection.results) {
			cv::Rect rect = result.location;

			//TODO: Age and Gender Detection 6

			// Giving same colour to male and female
			auto rectColor = cv::Scalar(0, 255, 0);

			cv::rectangle(frame, result.location, rectColor, 1);			
		}

		if (-1 != cv::waitKey(1))
			break;

		cv::imshow("Detection results", frame);

		if (!cap.retrieve(frame)) {
			break;
		}
		//TODO: Cloud integration 2
	}
	return 0;
}

                </ui-codemirror>
              </div>
            </div>         
        </div>

    
    
	 <div ibox title="Build the solution and observe the output">
	 <p>Execute the generated exe file to see the face detection output. The user has to use their mobile phones and capture more pictures to detect the faces.</p>
	 <div class="alert alert-warning">Go to <span style="color:#A52A2A">C:\Users\intel#\Desktop\Retail\OpenVINO\bin\intel64\Debug interactive_face_detection_sample</span></div>
                   
                                      <img                 src="../views/labs/analytics-openvinoagegenderdetection/images/openvinoface.jpg" />
                    
					
    </div>

    <div ibox title="Lesson learnt">
        <div content-block name="xdk-issues" message="Check after completion of lesson">
          <p>Face detection using Intel&reg; OpenVINO&trade; toolkit.</p>
        </div>
    </div>

</div>
