<div class="row wrapper border-bottom white-bg page-heading">
    <div class="col-lg-10">
        <h2>Age and Gender detection using OpenVINO&trade;</h2>
        <ol class="breadcrumb">
            <li>
                <a href="index.html">Home</a>
            </li>
            <li>
                <a>Labs</a>
            </li>
            <li>
                OpenVINO&trade; Sample
            </li>
            <li class="active">
                <strong>Age and Gender detection using OpenVINO&trade;</strong>
            </li>
        </ol>
    </div>
</div>

<div class="wrapper wrapper-content animated fadeInRight" ng-controller="CodeEditorCtrl">
          <div ibox title="Objectives">
		  <div content-block name="xdk-issues" message="Check after completion of Lab Overview" image-link="../views/labs/analytics-openvinoagegenderdetection/images/ClassDiagram.png">
             
              <b>Lab Overview</b>
              <ul>
			  <li>We have done face detection in our previous module. Now, we will identify Age and Gender for the identified faces.</li>
			  <li>We will build upon our face detection code and add Age, Gender identification code in this module.</li>
			  </ul>
			  
			</div>	
             <div content-block name="xdk-issues" message="Check after understanding task for this Lab" image-link="../views/labs/analytics-openvinoagegenderdetection/images/FlowChart.png">   
           <p>We will be replacing Age and Gender detection TODOs with the following:</p>
			<ul>
			<li>We will include CPU as plugin device for parallel inferencing.</li>
			<li>Load pre-trained data model for Age and Gender detection</li>
			<li>Once face is detection result is available, we ill submit inference request for Age and Gender detection</li>
			<li>We will marked the identified faces inside rectangle and put text on it for Age and Gender</li>
			<li>Here we will observe age and gender detection in addition to face.</li>
			</ul>              
              </div>
          </div>
          <div ibox title="Include CPU as plugin device">
                <div content-block name="xdk-issues" message="Check after including Header files">
                <p>We need to include CPU as plugin device for inferencing Age and Gender</p>  
				  <ul>
					<li>Replace second #TODO: Age and Gender detection 2</li>
					<li>Paste the following lines</li>
					</ul>
                    
				 <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">					
					plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("CPU");
					pluginsForDevices["CPU"] = plugin;
                    </ui-codemirror>
              </div>
            </div>

          <div ibox title="Load pre-trained optimize model for Age and Gender inferencing">
                <div content-block name="xdk-issues" message="Check after loading optimized model on CPU">
				<p>We need CPU as plugin device for inferencing Age and Gender and load pre retained model for Age and Gender detection on CPU</p>
				 <ul>
					<li>Replace third #TODO: Age and Gender detection 3</li>
					<li>Paste the following lines</li>
					</ul>                  
                  <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
                    FLAGS_Age_Gender_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\  
                                         intel_models\\age-gender-recognition-retail-0013\\FP32\\ age-gender- 
                                          recognition-retail-0013.xml";
					AgeGenderDetection AgeGender;
					Load(AgeGender).into(pluginsForDevices["CPU"]);
                </ui-codemirror>
              
              </div>
            </div>

          <div ibox title="Submit Inference Request">
                <div content-block name="xdk-issues" message="Check after submiting inference request">
				<p>We have detected face in our previous lab. Here, we will submit inference request identify Age and Gender for the identified face</p>
				<ul>
					<li>Replace fourth #TODO: Age and Gender detection 4</li>
					<li>Paste the following lines</li>
					</ul>
                  </br>
                    <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
                   //Submit Inference Request for age and gender detection and wait for result
	               AgeGender.submitRequest();
	               AgeGender.wait();
                    </ui-codemirror>
                  </div>
            </div>
			<div ibox title="Use idenfied face for Age and Gender detection">
			<p>Clipped the identified faces and send inference request for identifying age and gender</p>
			<ul>
				<li>Replace fifth #TODO: Age and Gender detection 5</li>
				<li>Paste the following lines</li>
			</ul>
                <div content-block name="xdk-issues" message="Check after using identified face for Age and Gender detection">
                  </br>
                    <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
                    //Clipped the identified face and send Inference Request for age and gender detection
					for (auto face : FaceDetection.results) {
						auto clippedRect = face.location & cv::Rect(0, 0, 640, 480);
						auto face = frame(clippedRect);
						AgeGender.enqueue(face);
					}
					// Got the Face, Age and Gender detection result, now customize and print them on window
					std::ostringstream out;
					index = 0;
					curFaceCount = 0;
                    </ui-codemirror>
                  </div>
            </div>

          <div ibox title="Customize the result for display">
		  <p>Now we got result for face, age and gender detection. We will customize the output and display on screen</p>
		  <ul>
			<li>Replace sixth #TODO: Age and Gender detection 6</li>
			<li>Paste the following lines</li>			
		</ul>
                <div content-block name="xdk-issues" message="Check after customizing the result">
                </br>
                  <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
					out.str("");
					curFaceCount++;

					//Draw rectangle bounding identified face and print Age and Gender
					out << (AgeGender[index].maleProb > 0.5 ? "M" : "F");
					out << "," << static_cast<int>(AgeGender[index].age);

					cv::putText(frame,
					out.str(),
					cv::Point2f(result.location.x, result.location.y - 15),
					cv::FONT_HERSHEY_COMPLEX_SMALL,
					0.8,
					cv::Scalar(0, 0, 255));

					index++;
                </ui-codemirror>
              </div>
            </div>
			<div ibox title="Here is the final solution">
		  <p>Keep the TODOs as it is. We will re-use this program during Cloud Integration.</p>
		  
                <div content-block name="xdk-issues" message="Check after completion of Main inferencing loop">
                </br>
                  <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
#include &ltgflags/gflags.h&gt
#include &ltfunctional&gt
#include &ltiostream&gt
#include &ltfstream&gt
#include &ltrandom&gt
#include &ltmemory&gt
#include &ltchrono&gt
#include &ltstring&gt
#include &ltutility&gt
#include &ltalgorithm&gt
#include &ltiterator&gt
#include &ltsamples/common.hpp&gt
#include &ltsamples/slog.hpp&gt
#include &ltext_list.hpp&gt
#include &ltsstream&gt
#include &ltmap&gt
#include &ltvector&gt
#include "mkldnn/mkldnn_extension_ptr.hpp"
#include &ltinference_engine.hpp&gt
#include "interactive_face_detection.hpp"
#include &ltopencv2/opencv.hpp&gt

using namespace InferenceEngine;


template &lttypename T&gt
void matU8ToBlob(const cv::Mat& orig_image, Blob::Ptr& blob, float scaleFactor = 1.0, int batchIndex = 0) {
	SizeVector blobSize = blob.get()->dims();
	const size_t width = blobSize[0];
	const size_t height = blobSize[1];
	const size_t channels = blobSize[2];

	T* blob_data = blob->buffer().as<T*>();

	cv::Mat resized_image(orig_image);
	if (width != orig_image.size().width || height != orig_image.size().height) {
		cv::resize(orig_image, resized_image, cv::Size(width, height));
	}

	int batchOffset = batchIndex * width * height * channels;

	for (size_t c = 0; c < channels; c++) {
		for (size_t h = 0; h < height; h++) {
			for (size_t w = 0; w < width; w++) {
				blob_data[batchOffset + c * width * height + h * width + w] =
					resized_image.at<cv::Vec3b>(h, w)[c] * scaleFactor;
			}
		}
	}
}

// -------------------------Generic routines for detection networks-------------------------------------------------

struct BaseDetection {
	ExecutableNetwork net;
	InferenceEngine::InferencePlugin * plugin;
	InferRequest::Ptr request;
	std::string & commandLineFlag;
	std::string topoName;
	const int maxBatch;

	BaseDetection(std::string &commandLineFlag, std::string topoName, int maxBatch)
		: commandLineFlag(commandLineFlag), topoName(topoName), maxBatch(maxBatch) {}

	virtual ~BaseDetection() {}

	ExecutableNetwork* operator ->() {
		return &net;
	}
	virtual InferenceEngine::CNNNetwork read() = 0;

	virtual void submitRequest() {
		if (!enabled() || request == nullptr) return;
		request->StartAsync();
	}

	virtual void wait() {
		if (!enabled() || !request) return;
		request->Wait(IInferRequest::WaitMode::RESULT_READY);
	}
	mutable bool enablingChecked = false;
	mutable bool _enabled = false;

	bool enabled() const {
		if (!enablingChecked) {
			_enabled = !commandLineFlag.empty();

			enablingChecked = true;
		}
		return _enabled;
	}
	void printPerformanceCounts() {
		if (!enabled()) {
			return;
		}

	}
};

struct FaceDetectionClass : BaseDetection {
	std::string input;
	std::string output;
	int maxProposalCount;
	int objectSize;
	int enquedFrames = 0;
	float width = 0;
	float height = 0;
	bool resultsFetched = false;
	std::vector<std::string> labels;
	using BaseDetection::operator=;

	struct Result {
		int label;
		float confidence;
		cv::Rect location;
	};

	std::vector<Result> results;

	void submitRequest() override {
		if (!enquedFrames) return;
		enquedFrames = 0;
		resultsFetched = false;
		results.clear();
		BaseDetection::submitRequest();
	}

	void enqueue(const cv::Mat &frame) {
		//if (!enabled()) return;

		if (!request) {
			request = net.CreateInferRequestPtr();
		}

		width = frame.cols;
		height = frame.rows;

		auto  inputBlob = request->GetBlob(input);

		matU8ToBlob<uint8_t >(frame, inputBlob);

		enquedFrames = 1;
	}

	FaceDetectionClass() : BaseDetection(FLAGS_Face_Model, "Face Detection", 1) {}
	InferenceEngine::CNNNetwork read() override {
		InferenceEngine::CNNNetReader netReader;
		/** Read network model **/
		netReader.ReadNetwork(FLAGS_Face_Model);
		/** Set batch size to 1 **/
		netReader.getNetwork().setBatchSize(maxBatch);
		/** Extract model name and load it's weights **/
		std::string binFileName = fileNameNoExt(FLAGS_Face_Model) + ".bin";
		netReader.ReadWeights(binFileName);
		/** Read labels (if any)**/
		std::string labelFileName = fileNameNoExt(FLAGS_Face_Model) + ".labels";

		std::ifstream inputFile(labelFileName);
		std::copy(std::istream_iterator<std::string>(inputFile),
			std::istream_iterator<std::string>(),
			std::back_inserter(labels));

		/** SSD-based network should have one input and one output **/
		// ---------------------------Check inputs ------------------------------------------------------
		InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());
		auto& inputInfoFirst = inputInfo.begin()->second;
		inputInfoFirst->setPrecision(Precision::U8);
		inputInfoFirst->getInputData()->setLayout(Layout::NCHW);

		// ---------------------------Check outputs ------------------------------------------------------
		InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

		auto& _output = outputInfo.begin()->second;
		output = outputInfo.begin()->first;

		const auto outputLayer = netReader.getNetwork().getLayerByName(output.c_str());

		const int num_classes = outputLayer->GetParamAsInt("num_classes");

		const InferenceEngine::SizeVector outputDims = _output->dims;
		maxProposalCount = outputDims[1];
		objectSize = outputDims[0];

		_output->setPrecision(Precision::FP32);
		_output->setLayout(Layout::NCHW);


		input = inputInfo.begin()->first;
		return netReader.getNetwork();
	}

	void fetchResults() {
		if (!enabled()) return;
		results.clear();
		if (resultsFetched) return;
		resultsFetched = true;
		const float *detections = request->GetBlob(output)->buffer().as<float *>();

		for (int i = 0; i < maxProposalCount; i++) {
			float image_id = detections[i * objectSize + 0];
			Result r;
			r.label = static_cast<int>(detections[i * objectSize + 1]);
			r.confidence = detections[i * objectSize + 2];
			if (r.confidence <= FLAGS_t) {
				continue;
			}

			r.location.x = detections[i * objectSize + 3] * width;
			r.location.y = detections[i * objectSize + 4] * height;
			r.location.width = detections[i * objectSize + 5] * width - r.location.x;
			r.location.height = detections[i * objectSize + 6] * height - r.location.y;

			if (image_id < 0) {
				break;
			}
			if (FLAGS_r) {
				std::cout << "[" << i << "," << r.label << "] element, prob = " << r.confidence <<
					"    (" << r.location.x << "," << r.location.y << ")-(" << r.location.width << ","
					<< r.location.height << ")"
					<< ((r.confidence > FLAGS_t) ? " WILL BE RENDERED!" : "") << std::endl;
			}

			results.push_back(r);
		}
	}
};

struct AgeGenderDetection : BaseDetection {
	std::string input;
	std::string outputAge;
	std::string outputGender;
	int enquedFaces = 0;

	using BaseDetection::operator=;
	AgeGenderDetection() : BaseDetection(FLAGS_Age_Gender_Model, "Age Gender", FLAGS_n_ag) {}

	void submitRequest() override {
		if (!enquedFaces) return;
		BaseDetection::submitRequest();
		enquedFaces = 0;
	}

	void enqueue(const cv::Mat &face) {

		if (!request) {
			request = net.CreateInferRequestPtr();
		}

		auto  inputBlob = request->GetBlob(input);
		matU8ToBlob<float>(face, inputBlob, 1.0f, enquedFaces);
		enquedFaces++;
	}

	struct Result { float age; float maleProb; };
	Result operator[] (int idx) const {
		auto  genderBlob = request->GetBlob(outputGender);
		auto  ageBlob = request->GetBlob(outputAge);

		return{ ageBlob->buffer().as<float*>()[idx] * 100,
			genderBlob->buffer().as<float*>()[idx * 2 + 1] };
	}

	CNNNetwork read() override {

		InferenceEngine::CNNNetReader netReader;
		/** Read network model **/
		netReader.ReadNetwork(FLAGS_Age_Gender_Model);

		//	/** Set batch size to 16 
		netReader.getNetwork().setBatchSize(16);
		//slog::info << "Batch size is set to " << netReader.getNetwork().getBatchSize() << " for Age Gender" << slog::endl;**/

		/** Extract model name and load it's weights **/
		std::string binFileName = fileNameNoExt(FLAGS_Age_Gender_Model) + ".bin";
		netReader.ReadWeights(binFileName);

		/** Age Gender network should have one input two outputs **/
		InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());

		auto& inputInfoFirst = inputInfo.begin()->second;
		inputInfoFirst->setPrecision(Precision::FP32);
		inputInfoFirst->getInputData()->setLayout(Layout::NCHW);
		input = inputInfo.begin()->first;

		// ---------------------------Check outputs ------------------------------------------------------
		InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

		auto it = outputInfo.begin();
		auto ageOutput = (it++)->second;
		auto genderOutput = (it++)->second;

		outputAge = ageOutput->name;
		outputGender = genderOutput->name;
		_enabled = true;
		return netReader.getNetwork();
	}
};

// ***** For loading xml and bin files
struct Load {
	BaseDetection& detector;
	explicit Load(BaseDetection& detector) : detector(detector) { }

	void into(InferenceEngine::InferencePlugin & plg) const {
		if (detector.enabled()) {
			detector.net = plg.LoadNetwork(detector.read(), {});
			detector.plugin = &plg;
		}
	}
};

int main(int argc, char *argv[]) {
	int faceCountThreshold = 100;
	int curFaceCount = 0;
	int prevFaceCount = 0;
	int index = 0;
	//TODO: Cloud integration 1

	//If there is a single camera connected, just pass 0.
	cv::VideoCapture cap;
	cap.open(0);

	cv::Mat frame;
	cap.read(frame);

	//Select plugins for inference engine
	std::map<std::string, InferencePlugin> pluginsForDevices;

	//Select GPU as plugin device to load Face Detection pre trained optimized model
	InferencePlugin plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("GPU");
	pluginsForDevices["GPU"] = plugin;

	//Select GPU as plugin device to load Age and Gender Detection pre trained optimized model
	plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("CPU");
	pluginsForDevices["CPU"] = plugin;


	//Load pre trained optimized data model for face detection 
	FLAGS_Face_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\intel_models\\face-detection-adas-0001\\FP32\\face-detection-adas-0001.xml";

	//Load Face Detection model to target device
	FaceDetectionClass FaceDetection;
	Load(FaceDetection).into(pluginsForDevices["GPU"]);


	//Load pre trained optimized data model for Age and Gender detection 
	FLAGS_Age_Gender_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\intel_models\\age-gender-recognition-retail-0013\\FP32\\age-gender-recognition-retail-0013.xml";
	AgeGenderDetection AgeGender;
	Load(AgeGender).into(pluginsForDevices["CPU"]);


	// Main inference loop	
	while (true) {
		//Grab the next frame from camera and populate Inference Request
		cap.grab();
		FaceDetection.enqueue(frame);

		//Submit Inference Request for face detection and wait for result
		FaceDetection.submitRequest();
		FaceDetection.wait();

		//Submit Inference Request for age and gender detection and wait for result
		AgeGender.submitRequest();
		AgeGender.wait();

		FaceDetection.fetchResults();
		//Clipped the identified face and send Inference Request for age and gender detection
		for (auto face : FaceDetection.results) {
			auto clippedRect = face.location & cv::Rect(0, 0, 640, 480);
			auto face = frame(clippedRect);
			AgeGender.enqueue(face);
		}
		// Got the Face, Age and Gender detection result, now customize and print them on window
		std::ostringstream out;
		index = 0;
		curFaceCount = 0;

		for (auto & result : FaceDetection.results) {
			cv::Rect rect = result.location;

			out.str("");
			curFaceCount++;

			//Draw rectangle bounding identified face and print Age and Gender
			out << (AgeGender[index].maleProb > 0.5 ? "M" : "F");
			out << "," << static_cast<int>(AgeGender[index].age);

			cv::putText(frame,
				out.str(),
				cv::Point2f(result.location.x, result.location.y - 15),
				cv::FONT_HERSHEY_COMPLEX_SMALL,
				0.8,
				cv::Scalar(0, 0, 255));

			index++;

			// Giving same colour to male and female
			auto rectColor = cv::Scalar(0, 255, 0);

			cv::rectangle(frame, result.location, rectColor, 1);			
		}

		if (-1 != cv::waitKey(1))
			break;

		cv::imshow("Detection results", frame);

		if (!cap.retrieve(frame)) {
			break;
		}
		//TODO: Cloud integration 2
	}
	return 0;
}

                </ui-codemirror>
              </div>
            </div>         
        </div>    
	 <div ibox title="Build the solution and analyze the output">
	 <p>Execute the generated exe file to see the face detection output. The user has to use their mobile phones and capture more pictures to detect the faces.</p>
	 <div class="alert alert-warning">Go to <span style="color:#A52A2A">C:\Users\intel#\Desktop\Retail\OpenVINO\bin\intel64\Debug interactive_face_detection_sample</span></div>                   
		<img                 src="../views/labs/analytics-openvinoagegenderdetection/images/openvinoface.jpg" />      			
    </div>

    <div ibox title="Lesson learnt">
        <div content-block name="xdk-issues" message="Check after completion of lesson">
          <p>In addition to Face, Age and Gender detection using Intel&reg; OpenVINO&trade; toolkit</p>
        </div>
    </div>

</div>
