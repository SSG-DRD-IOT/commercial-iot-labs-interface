<div class="row wrapper border-bottom white-bg page-heading">
    <div class="col-lg-10">
        <h2>Analyse face data on cloud</h2>
        <ol class="breadcrumb">
            <li>
                <a href="index.html">Home</a>
            </li>
            <li>
                <a>Labs</a>
            </li>
            <li>
                <strong>Video Analytics</strong>
            </li>
            <li class="active">
                <strong>Analyse face data on cloud</strong>
            </li>
        </ol>
    </div>
</div>

<div class="wrapper wrapper-content animated fadeInRight" ng-controller="CodeEditorCtrl">
      <div class="row">
        <div class="col-lg-12">

            <div ibox title="Objectives">
                <div content-block name="opencv_motion-objectives" message="Complete Objectives" image-link="../views/labs/analytics-analyse/images/cloudmain.png">
                    <h2 class="labHidden"></h2>
                    <h5>Lab Overview</h5>
                    <p>We have done face, age and gender detection in our previous modules. We have successfully counted number of faces so far.
                    </p>
                    <p>In this module we will publish this data to local cloud for analysis</p>
                    <p>We will include these following  points</p>
                    <ul>
                        <li>We will declare a device id that will be used for publishing the data to cloud</li>
						<li>We will integrate a python script for publishing the data to cloud</li>
                        <li>We will publish the number of faces after showing the face count</li>
                        <li>Login to cloud and view charts showing the number of faces</li>
                    </ul>
              </div>
            </div>
            <div ibox title="Declare the device id">

              <div content-block name="opencv_package-newfile" message="Declare the device id">
               
                  <ol>
                <li>Replace first #TODO: Cloud integration 1</li>
                <li>Paste the following line and replace the device id “1234” with your device id written on your computer.</li>
                <li><pre><code>std::string deviceId = “1234”;
</code></pre></li>
                       </ol>
              </div>
                </div>

                <div ibox title="Publish number of faces to cloud">

                  <div content-block name="opencv_package-cloud" message="Publish number of faces to cloud">
                    <h5>We have counted number of faces in the sceen. We need to publish this counter values to cloud. </h5>
                    <p>Note: We are not publishing video stream or pictures of the sceen. We are only publishing the number of faces. For publishing the data to cloud we will be integrating a python script.</p>
                      <ol>
                    <li>The following content should be present in a python script called as “cloud.py” and copied to the Desktop &gt; Retail &gt; OpenVINO</li>
                    <li>You need to add below python script (cloud.py) to your system.</li>
                                       						
				 
                     <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
					import requests
					import sys
					id = sys.argv[1]
					facecount = sys.argv[2]
					print(id);
					print(facecount);
					query = 'id' + '&value=' + str(facecount);
					print(query);
					resp = requests.get('http://10.138.77.101:9002/analytics/face?'+ query);

					if resp.status_code != 201:
						print("Unable to submit the data")
					else:
						print("Data Submitted for analysis")
						

 </ui-codemirror>
				 <li>Replace second #TODO: Cloud integration 2</li>
				 <li>Note that we are sending the data to cloud when there is a change is number of faces identified.</li>
				 <li>Paste the following line</li>
				 <ui-codemirror ui-codemirror-opts="editorOptions" id="topic-configure">
	//Submit data to cloud when there is change in face count
		if (curFaceCount != prevFaceCount && curFaceCount < faceCountThreshold)
		{
			prevFaceCount = curFaceCount;
			
			//Integrate python module to submit data to cloud
			std::string cmd = "python C:\\Users\\intel1672\\Desktop\\Retail\\OpenVINO\\cloud.py " + id + " " + std::to_string(curFaceCount);
			std::system(cmd.c_str());

			slog::info << "Number of faces in the frame are : " << curFaceCount << slog::endl;
		}



 </ui-codemirror>
                           </ol>
			
                  </div>
				  
                    </div>
					 <div ibox title="Visualize Your data">
                <div content-block name="opencv_analyse-objectives" message="Visualize Your data" image-link="../views/labs/analytics-analyse/images/facecountoncloud.png">
                    <h2 class="labHidden"></h2>
                    <h5>Visualizing your data on the cloud</h5>
                    <ul>
                        <li>Go to http://&lt;cloud server url>:9002</li>
                        <li>Example: <a id="href1" target="_blank"></a><script>document.getElementById("href1").innerHTML = window.location.hostname + ":9002";document.getElementById("href1").href = "http://" + window.location.hostname + ":9002";</script></li>
                        <li>Enter your device ID</li>
                        <li>Click on plot</li>
						<li>See the real time face count on cloud</li>
						
                    </ul>
					
              </div>
            </div>
            <div ibox title="Here is the final solution">
              <div content-block name="opencv_motion-newfile" message="Here is the final solution">
                <h5>Here is the final code base including cloud integration</h5>
                  <ui-codemirror ui-codemirror-opts="editorOptions"><pre class="brush:jscript;">
#include &ltgflags/gflags.h&gt
#include &ltfunctional&gt
#include &ltiostream&gt
#include &ltfstream&gt
#include &ltrandom&gt
#include &ltmemory&gt
#include &ltvector&gt
#include &ltstring&gt
#include &ltutility&gt
#include &ltalgorithm&gt
#include &ltiterator&gt
#include &ltmap&gt
#include &ltinference_engine.hpp&gt
#include &ltsamples/common.hpp&gt
#include &ltsamples/slog.hpp&gt
#include "interactive_face_detection.hpp"
#include "mkldnn/mkldnn_extension_ptr.hpp"
#include &ltext_list.hpp&gt
#include &ltsstream&gt

#include &ltopencv2/opencv.hpp&gt

using namespace InferenceEngine;


template <typename T>
void matU8ToBlob(const cv::Mat& orig_image, Blob::Ptr& blob, float scaleFactor = 1.0, int batchIndex = 0) {
	SizeVector blobSize = blob.get()->dims();
	const size_t width = blobSize[0];
	const size_t height = blobSize[1];
	const size_t channels = blobSize[2];

	T* blob_data = blob->buffer().as<T*>();

	cv::Mat resized_image(orig_image);
	if (width != orig_image.size().width || height != orig_image.size().height) {
		cv::resize(orig_image, resized_image, cv::Size(width, height));
	}

	int batchOffset = batchIndex * width * height * channels;

	for (size_t c = 0; c < channels; c++) {
		for (size_t h = 0; h < height; h++) {
			for (size_t w = 0; w < width; w++) {
				blob_data[batchOffset + c * width * height + h * width + w] =
					resized_image.at<cv::Vec3b>(h, w)[c] * scaleFactor;
			}
		}
	}
}

// -------------------------Generic routines for detection networks-------------------------------------------------

struct BaseDetection {
	ExecutableNetwork net;
	InferenceEngine::InferencePlugin * plugin;
	InferRequest::Ptr request;
	std::string & commandLineFlag;
	std::string topoName;
	const int maxBatch;

	BaseDetection(std::string &commandLineFlag, std::string topoName, int maxBatch)
		: commandLineFlag(commandLineFlag), topoName(topoName), maxBatch(maxBatch) {}

	virtual ~BaseDetection() {}

	ExecutableNetwork* operator ->() {
		return &net;
	}
	virtual InferenceEngine::CNNNetwork read() = 0;

	virtual void submitRequest() {
		if (!enabled() || request == nullptr) return;
		request->StartAsync();
	}

	virtual void wait() {
		if (!enabled() || !request) return;
		request->Wait(IInferRequest::WaitMode::RESULT_READY);
	}
	mutable bool enablingChecked = false;
	mutable bool _enabled = false;

	bool enabled() const {
		if (!enablingChecked) {
			_enabled = !commandLineFlag.empty();

			enablingChecked = true;
		}
		return _enabled;
	}
	void printPerformanceCounts() {
		if (!enabled()) {
			return;
		}

	}
};

struct FaceDetectionClass : BaseDetection {
	std::string input;
	std::string output;
	int maxProposalCount;
	int objectSize;
	int enquedFrames = 0;
	float width = 0;
	float height = 0;
	bool resultsFetched = false;
	std::vector<std::string> labels;
	using BaseDetection::operator=;

	struct Result {
		int label;
		float confidence;
		cv::Rect location;
	};

	std::vector<Result> results;

	void submitRequest() override {
		if (!enquedFrames) return;
		enquedFrames = 0;
		resultsFetched = false;
		results.clear();
		BaseDetection::submitRequest();
	}

	void enqueue(const cv::Mat &frame) {
		//if (!enabled()) return;

		if (!request) {
			request = net.CreateInferRequestPtr();
		}

		width = frame.cols;
		height = frame.rows;

		auto  inputBlob = request->GetBlob(input);

		matU8ToBlob<uint8_t >(frame, inputBlob);

		enquedFrames = 1;
	}

	FaceDetectionClass() : BaseDetection(FLAGS_Face_Model, "Face Detection", 1) {}
	InferenceEngine::CNNNetwork read() override {
		InferenceEngine::CNNNetReader netReader;
		/** Read network model **/
		netReader.ReadNetwork(FLAGS_Face_Model);
		/** Set batch size to 1 **/
		netReader.getNetwork().setBatchSize(maxBatch);
		/** Extract model name and load it's weights **/
		std::string binFileName = fileNameNoExt(FLAGS_Face_Model) + ".bin";
		netReader.ReadWeights(binFileName);
		/** Read labels (if any)**/
		std::string labelFileName = fileNameNoExt(FLAGS_Face_Model) + ".labels";

		std::ifstream inputFile(labelFileName);
		std::copy(std::istream_iterator<std::string>(inputFile),
			std::istream_iterator<std::string>(),
			std::back_inserter(labels));

		/** SSD-based network should have one input and one output **/
		// ---------------------------Check inputs ------------------------------------------------------
		InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());
		auto& inputInfoFirst = inputInfo.begin()->second;
		inputInfoFirst->setPrecision(Precision::U8);
		inputInfoFirst->getInputData()->setLayout(Layout::NCHW);

		// ---------------------------Check outputs ------------------------------------------------------
		InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

		auto& _output = outputInfo.begin()->second;
		output = outputInfo.begin()->first;

		const auto outputLayer = netReader.getNetwork().getLayerByName(output.c_str());

		const int num_classes = outputLayer->GetParamAsInt("num_classes");

		const InferenceEngine::SizeVector outputDims = _output->dims;
		maxProposalCount = outputDims[1];
		objectSize = outputDims[0];

		_output->setPrecision(Precision::FP32);
		_output->setLayout(Layout::NCHW);


		input = inputInfo.begin()->first;
		return netReader.getNetwork();
	}

	void fetchResults() {
		if (!enabled()) return;
		results.clear();
		if (resultsFetched) return;
		resultsFetched = true;
		const float *detections = request->GetBlob(output)->buffer().as<float *>();

		for (int i = 0; i < maxProposalCount; i++) {
			float image_id = detections[i * objectSize + 0];
			Result r;
			r.label = static_cast<int>(detections[i * objectSize + 1]);
			r.confidence = detections[i * objectSize + 2];
			if (r.confidence <= FLAGS_t) {
				continue;
			}

			r.location.x = detections[i * objectSize + 3] * width;
			r.location.y = detections[i * objectSize + 4] * height;
			r.location.width = detections[i * objectSize + 5] * width - r.location.x;
			r.location.height = detections[i * objectSize + 6] * height - r.location.y;

			if (image_id < 0) {
				break;
			}
			if (FLAGS_r) {
				std::cout << "[" << i << "," << r.label << "] element, prob = " << r.confidence <<
					"    (" << r.location.x << "," << r.location.y << ")-(" << r.location.width << ","
					<< r.location.height << ")"
					<< ((r.confidence > FLAGS_t) ? " WILL BE RENDERED!" : "") << std::endl;
			}

			results.push_back(r);
		}
	}
};

struct AgeGenderDetection : BaseDetection {
	std::string input;
	std::string outputAge;
	std::string outputGender;
	int enquedFaces = 0;

	using BaseDetection::operator=;
	AgeGenderDetection() : BaseDetection(FLAGS_Age_Gender_Model, "Age Gender", FLAGS_n_ag) {}

	void submitRequest() override {
		if (!enquedFaces) return;
		BaseDetection::submitRequest();
		enquedFaces = 0;
	}

	void enqueue(const cv::Mat &face) {

		if (!request) {
			request = net.CreateInferRequestPtr();
		}

		auto  inputBlob = request->GetBlob(input);
		matU8ToBlob<float>(face, inputBlob, 1.0f, enquedFaces);
		enquedFaces++;
	}

	struct Result { float age; float maleProb; };
	Result operator[] (int idx) const {
		auto  genderBlob = request->GetBlob(outputGender);
		auto  ageBlob = request->GetBlob(outputAge);

		return{ ageBlob->buffer().as<float*>()[idx] * 100,
			genderBlob->buffer().as<float*>()[idx * 2 + 1] };
	}

	CNNNetwork read() override {

		InferenceEngine::CNNNetReader netReader;
		/** Read network model **/
		netReader.ReadNetwork(FLAGS_Age_Gender_Model);

		//	/** Set batch size to 16 
		netReader.getNetwork().setBatchSize(16);
		//slog::info << "Batch size is set to " << netReader.getNetwork().getBatchSize() << " for Age Gender" << slog::endl;**/

		/** Extract model name and load it's weights **/
		std::string binFileName = fileNameNoExt(FLAGS_Age_Gender_Model) + ".bin";
		netReader.ReadWeights(binFileName);

		/** Age Gender network should have one input two outputs **/
		InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());

		auto& inputInfoFirst = inputInfo.begin()->second;
		inputInfoFirst->setPrecision(Precision::FP32);
		inputInfoFirst->getInputData()->setLayout(Layout::NCHW);
		input = inputInfo.begin()->first;

		// ---------------------------Check outputs ------------------------------------------------------
		InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

		auto it = outputInfo.begin();
		auto ageOutput = (it++)->second;
		auto genderOutput = (it++)->second;

		outputAge = ageOutput->name;
		outputGender = genderOutput->name;
		_enabled = true;
		return netReader.getNetwork();
	}
};

// ***** For loading xml and bin files
struct Load {
	BaseDetection& detector;
	explicit Load(BaseDetection& detector) : detector(detector) { }

	void into(InferenceEngine::InferencePlugin & plg) const {
		if (detector.enabled()) {
			detector.net = plg.LoadNetwork(detector.read(), {});
			detector.plugin = &plg;
		}
	}
};

int main(int argc, char *argv[]) {
	int faceCountThreshold = 100;
	int curFaceCount = 0;
	int prevFaceCount = 0;
	int index = 0;
	std::string id = "1234";

	//If there is a single camera connected, just pass 0.
	cv::VideoCapture cap;
	cap.open(0);

	cv::Mat frame;
	cap.read(frame);

	//Select plugins for inference engine
	std::map<std::string, InferencePlugin> pluginsForDevices;

	//Select GPU as plugin device to load Face Detection pre trained optimized model
	InferencePlugin plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("GPU");
	pluginsForDevices["GPU"] = plugin;

	//Select GPU as plugin device to load Age and Gender Detection pre trained optimized model
	plugin = PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("CPU");
	pluginsForDevices["CPU"] = plugin;


	//Load pre trained optimized data model for face detection 
	FLAGS_Face_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\intel_models\\face-detection-adas-0001\\FP32\\face-detection-adas-0001.xml";

	//Load Face Detection model to target device
	FaceDetectionClass FaceDetection;
	Load(FaceDetection).into(pluginsForDevices["GPU"]);


	//Load pre trained optimized data model for Age and Gender detection 
	FLAGS_Age_Gender_Model = "C:\\Intel\\computer_vision_sdk_2018.1.265\\deployment_tools\\intel_models\\age-gender-recognition-retail-0013\\FP32\\age-gender-recognition-retail-0013.xml";
	AgeGenderDetection AgeGender;
	Load(AgeGender).into(pluginsForDevices["CPU"]);


	// Main inference loop	
	while (true) {
		//Grab the next frame from camera and populate Inference Request
		cap.grab();
		FaceDetection.enqueue(frame);

		//Submit Inference Request for face detection and wait for result
		FaceDetection.submitRequest();
		FaceDetection.wait();

		//Submit Inference Request for age and gender detection and wait for result
		AgeGender.submitRequest();
		AgeGender.wait();

		FaceDetection.fetchResults();
		//Clipped the identified face and send Inference Request for age and gender detection
		for (auto face : FaceDetection.results) {
			auto clippedRect = face.location & cv::Rect(0, 0, 640, 480);
			auto face = frame(clippedRect);
			AgeGender.enqueue(face);
		}
		// Got the Face, Age and Gender detection result, now customize and print them on window
		std::ostringstream out;
		index = 0;
		curFaceCount = 0;

		for (auto & result : FaceDetection.results) {
			cv::Rect rect = result.location;

			out.str("");
			curFaceCount++;

			//Draw rectangle bounding identified face and print Age and Gender
			out << (AgeGender[index].maleProb > 0.5 ? "M" : "F");
			out << "," << static_cast<int>(AgeGender[index].age);

			cv::putText(frame,
				out.str(),
				cv::Point2f(result.location.x, result.location.y - 15),
				cv::FONT_HERSHEY_COMPLEX_SMALL,
				0.8,
				cv::Scalar(0, 0, 255));

			index++;

			// Giving same colour to male and female
			auto rectColor = cv::Scalar(0, 255, 0);

			cv::rectangle(frame, result.location, rectColor, 1);			
		}

		if (-1 != cv::waitKey(1))
			break;

		cv::imshow("Detection results", frame);

		if (!cap.retrieve(frame)) {
			break;
		}

		//Submit data to cloud when there is change in face count
		if (curFaceCount != prevFaceCount && curFaceCount < faceCountThreshold)
		{
			prevFaceCount = curFaceCount;
			
			//Integrate python module to submit data to cloud
			std::string cmd = "python C:\\Users\\intel1672\\Desktop\\Retail\\OpenVINO\\cloud.py " + id + " " + std::to_string(curFaceCount);
			std::system(cmd.c_str());

			slog::info << "Number of faces in the frame are : " << curFaceCount << slog::endl;
		}
	}
	return 0;
}



</pre></ui-codemirror>
              </div>
            </div>
			 <div ibox title="Lesson learnt">
        <div content-block name="xdk-issues" message="Check after completion of lesson">
          <p>Interfacing Intel&reg; OpenVINO™ toolkit with cloud and visualizing data on cloud.</p>
        </div>
    </div>
            
        <div ibox title="References">
          <ul>
              <li>
                  <p><a href="http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html" target="_blank">OpenVINO&trade; Tutorials</a></p></li><li>
                  <p><a href="https://pythonprogramming.net/loading-images-python-opencv-tutorial/" target="_blank">OpenVINO&trade;</a></p>
              </li>
          </ul>
        </div>


      </div>
  </div>

        </div>
    </div>
</div>
